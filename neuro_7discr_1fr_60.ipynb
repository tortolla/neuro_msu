{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e057f8f5-5133-4701-8505-797de7af00e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda:2\" if torch.cuda.is_available() else \"cpu\")\n",
    "num_epochs = 50\n",
    "k_folds = 4\n",
    "img_dir = 'hard_mode_time:60'\n",
    "workspace = 'exp-4-try'\n",
    "project_name='60-1fr'\n",
    "img_size = 600\n",
    "n_trials = 500\n",
    "filename = '6000'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9d9deeda-b6b5-46a6-80e7-149c210a954b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import transforms\n",
    "from tqdm import tqdm  # Импортируем tqdm для отображения прогресс-бара\n",
    "\n",
    "import os\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import transforms\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class CirclesSequenceDataset(Dataset):\n",
    "    def __init__(self, img_dir, img_size, transform=None, num_images=4375, discret=7, sequence_length=1, prediction_length=35):\n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transform or transforms.ToTensor()\n",
    "        self.img_size = img_size\n",
    "        self.discret = discret\n",
    "        self.sequence_length = sequence_length\n",
    "        self.prediction_length = prediction_length\n",
    "\n",
    "        all_imgs = sorted(os.listdir(img_dir), key=lambda x: int(x.split('_')[0]))\n",
    "        if num_images:\n",
    "            self.imgs = all_imgs[6:num_images:self.discret]\n",
    "        else:\n",
    "            self.imgs = all_imgs[6::self.discret]\n",
    "\n",
    "        if len(self.imgs) < self.sequence_length + self.prediction_length:\n",
    "            raise ValueError(f\"Недостаточно изображений для создания последовательностей (минимум {self.sequence_length + self.prediction_length} изображений).\")\n",
    "\n",
    "        self.cached_images = [None] * len(self.imgs)\n",
    "\n",
    "        print(\"Кэширование изображений:\")\n",
    "        for idx in tqdm(range(len(self.imgs)), desc=\"Прогресс кэширования\"):\n",
    "            self._load_image(idx)\n",
    "\n",
    "    def _load_image(self, idx):\n",
    "        if self.cached_images[idx] is None:\n",
    "            img_path = os.path.join(self.img_dir, self.imgs[idx])\n",
    "            image = Image.open(img_path).convert(\"L\")\n",
    "            self.cached_images[idx] = self.transform(image)\n",
    "        return self.cached_images[idx]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.imgs) - self.sequence_length - self.prediction_length + 1\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if idx + self.sequence_length + self.prediction_length > len(self.imgs):\n",
    "            raise IndexError(\"Индекс выходит за пределы списка изображений.\")\n",
    "\n",
    "        # Получаем последовательность изображений\n",
    "        imgs_sequence = [self._load_image(idx + i) for i in range(self.sequence_length)]\n",
    "        imgs_sequence = torch.stack(imgs_sequence)\n",
    "\n",
    "        # Собираем координаты меток в один тензор\n",
    "        coords = []\n",
    "        for i in range(self.prediction_length):\n",
    "            label_img_name = self.imgs[idx + self.sequence_length + i]\n",
    "            coord = label_img_name.split('.')[0].split('_')[1:]\n",
    "            coords.extend([float(c) / self.img_size for c in coord])\n",
    "\n",
    "        coords_tensor = torch.tensor(coords, dtype=torch.float32)\n",
    "\n",
    "        return imgs_sequence, coords_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dac7caa5-b32c-4930-b97d-385d0dcfaaff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "import timm\n",
    "import numpy as np\n",
    "from sklearn.metrics import r2_score, mean_absolute_error, mean_absolute_percentage_error\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5], std=[0.5]),  \n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7b70c290-61c1-4e15-a378-46eebb360eff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Кэширование изображений:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Прогресс кэширования: 100%|███████████████████| 625/625 [01:22<00:00,  7.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([[[[-1., -1., -1.,  ..., -1., -1., -1.],\n",
      "          [-1., -1., -1.,  ..., -1., -1., -1.],\n",
      "          [-1., -1., -1.,  ..., -1., -1., -1.],\n",
      "          ...,\n",
      "          [-1., -1., -1.,  ..., -1., -1., -1.],\n",
      "          [-1., -1., -1.,  ..., -1., -1., -1.],\n",
      "          [-1., -1., -1.,  ..., -1., -1., -1.]]]]), tensor([0.2950, 0.2900, 0.5983, 0.3333, 0.2417, 0.5750, 0.3000, 0.3000, 0.6100,\n",
      "        0.3317, 0.2367, 0.5650, 0.3067, 0.3100, 0.6217, 0.3300, 0.2333, 0.5550,\n",
      "        0.3150, 0.3183, 0.6333, 0.3283, 0.2283, 0.5433, 0.3250, 0.3250, 0.6433,\n",
      "        0.3300, 0.2267, 0.5317, 0.3333, 0.3317, 0.6533, 0.3367, 0.2283, 0.5217,\n",
      "        0.3433, 0.3400, 0.6600, 0.3450, 0.2333, 0.5100, 0.3483, 0.3483, 0.6683,\n",
      "        0.3550, 0.2433, 0.5067, 0.3467, 0.3600, 0.6750, 0.3633, 0.2517, 0.4983,\n",
      "        0.3433, 0.3717, 0.6850, 0.3683, 0.2583, 0.4883, 0.3450, 0.3833, 0.6967,\n",
      "        0.3717, 0.2650, 0.4783, 0.3467, 0.3950, 0.7067, 0.3767, 0.2717, 0.4700,\n",
      "        0.3533, 0.4000, 0.7183, 0.3800, 0.2683, 0.4683, 0.3617, 0.3917, 0.7300,\n",
      "        0.3800, 0.2583, 0.4750, 0.3700, 0.3833, 0.7417, 0.3767, 0.2483, 0.4800,\n",
      "        0.3767, 0.3750, 0.7517, 0.3733, 0.2383, 0.4867, 0.3800, 0.3650, 0.7633,\n",
      "        0.3717, 0.2267, 0.4883, 0.3783, 0.3550, 0.7750, 0.3683, 0.2150, 0.4900,\n",
      "        0.3717, 0.3450, 0.7867, 0.3700, 0.2033, 0.4900, 0.3617, 0.3383, 0.7967,\n",
      "        0.3650, 0.1917, 0.4900, 0.3517, 0.3333, 0.8033, 0.3567, 0.1817, 0.4950,\n",
      "        0.3417, 0.3267, 0.8100, 0.3467, 0.1733, 0.5033, 0.3417, 0.3150, 0.8167,\n",
      "        0.3367, 0.1667, 0.5117, 0.3433, 0.3033, 0.8233, 0.3283, 0.1683, 0.5217,\n",
      "        0.3417, 0.2917, 0.8317, 0.3183, 0.1717, 0.5333, 0.3367, 0.2817, 0.8283,\n",
      "        0.3067, 0.1750, 0.5450, 0.3317, 0.2717, 0.8267, 0.2950, 0.1750, 0.5550,\n",
      "        0.3267, 0.2600, 0.8250, 0.2833, 0.1683, 0.5650, 0.3183, 0.2533, 0.8233,\n",
      "        0.2733, 0.1617, 0.5750, 0.3167, 0.2433, 0.8167, 0.2633, 0.1567, 0.5850,\n",
      "        0.3183, 0.2317, 0.8100, 0.2533, 0.1467, 0.5917, 0.3150, 0.2200, 0.8033,\n",
      "        0.2433, 0.1350, 0.5917, 0.3150, 0.2083, 0.7933, 0.2400, 0.1233, 0.5900,\n",
      "        0.3183, 0.1983, 0.7817, 0.2417, 0.1133, 0.5950, 0.3183, 0.1867, 0.7717,\n",
      "        0.2433, 0.1033, 0.6000]))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader, random_split\n",
    "\n",
    "def get_data_loaders(dataset, batch_size, train_size=0.7):\n",
    "    train_len = int(len(dataset) * train_size)\n",
    "    val_len = len(dataset) - train_len\n",
    "\n",
    "    # Разделение на тренировочную и валидационную выборки\n",
    "    train_dataset, val_dataset = random_split(dataset, [train_len, val_len])\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, \n",
    "                          num_workers=40, pin_memory=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True, \n",
    "                          num_workers=40, pin_memory=True)\n",
    "\n",
    "    print(f\"Размер тренировочной выборки: {len(train_dataset)}\")\n",
    "    print(f\"Размер валидационной выборки: {len(val_dataset)}\")\n",
    "    \n",
    "    return train_loader, val_loader\n",
    "\n",
    "dataset = CirclesSequenceDataset(transform = transform, img_dir=img_dir, img_size=img_size)\n",
    "\n",
    "\n",
    "print(dataset[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84e5a261-0821-490a-a773-c90da85ffa49",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-11-17 22:03:26,538] A new study created in memory with name: no-name-af6d3a30-bf16-4a42-9c31-479e85db0ac2\n",
      "/tmp/ipykernel_1359663/2147759681.py:129: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler()  # Инициализация GradScaler для масштабирования градиентов\n",
      "  0%|                                                    | 0/19 [00:00<?, ?it/s]/tmp/ipykernel_1359663/2147759681.py:135: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():  # Включение смешанной точности\n",
      "/tmp/ipykernel_1359663/2147759681.py:129: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler()  # Инициализация GradScaler для масштабирования градиентов\n",
      "  0%|                                                    | 0/19 [00:00<?, ?it/s]/tmp/ipykernel_1359663/2147759681.py:135: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():  # Включение смешанной точности\n",
      "\u001b[1;38;5;214mCOMET WARNING:\u001b[0m To get all data logged automatically, import comet_ml before the following modules: sklearn, torch.\n",
      "\u001b[1;38;5;214mCOMET WARNING:\u001b[0m As you are running in a Jupyter environment, you will need to call `experiment.end()` when finished to ensure all metrics and code are logged before exiting.\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m Experiment is live on comet.com https://www.comet.com/exp-4-try/60-1fr/19c76dee765042f69f24cca47228ea51\n",
      "\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m Couldn't find a Git repository in '/home/maindev/new_life/for_server_gen' nor in any parent directory. Set `COMET_GIT_DIRECTORY` if your Git Repository is elsewhere.\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m ---------------------------------------------------------------------------------------\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m Comet.ml Experiment Summary\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m ---------------------------------------------------------------------------------------\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m   Data:\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     display_summary_level : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     name                  : confidential_sofa_5561\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     url                   : https://www.comet.com/exp-4-try/60-1fr/19c76dee765042f69f24cca47228ea51\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m   Metrics:\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     flops           : 518444576.0\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     mean_mae        : 0.14022991061210632\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     mean_mape       : 0.47740820050239563\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     mean_r2         : 0.23471405265570458\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     mean_train_loss : 0.029506986215710643\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     mean_val_loss   : 0.03032910662813712\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m   Parameters:\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     batch_size      : 16\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     dropout_rate    : 0.24276311362785996\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     filters_1       : 10\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     hidden_size     : 7\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     kernel_size_1   : 5\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     kernel_size_2   : 5\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     learning_rate   : 0.00034109308541877036\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     num_layers      : 3\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     padding_1       : 2\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     padding_2       : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     stride_1        : 2\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     stride_2        : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     use_second_conv : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m   Uploads:\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     environment details : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     filename            : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     installed packages  : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     notebook            : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     os packages         : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     source_code         : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m \n",
      "\u001b[1;38;5;214mCOMET WARNING:\u001b[0m To get all data logged automatically, import comet_ml before the following modules: sklearn, torch.\n",
      "[I 2024-11-17 22:05:02,728] Trial 0 finished with values: [0.47740820050239563, 518444576.0] and parameters: {'use_second_conv': False, 'batch_size': 16, 'filters_1': 10, 'hidden_size': 7, 'num_layers': 3, 'dropout_rate': 0.24276311362785996, 'kernel_size_1': 5, 'kernel_size_2': 5, 'stride_1': 2, 'stride_2': 1, 'padding_1': 2, 'padding_2': 1, 'learning_rate': 0.00034109308541877036}. \n",
      "/tmp/ipykernel_1359663/2147759681.py:129: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler()  # Инициализация GradScaler для масштабирования градиентов\n",
      "  0%|                                                    | 0/19 [00:00<?, ?it/s]/tmp/ipykernel_1359663/2147759681.py:135: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():  # Включение смешанной точности\n",
      "/tmp/ipykernel_1359663/2147759681.py:129: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler()  # Инициализация GradScaler для масштабирования градиентов\n",
      "  0%|                                                    | 0/19 [00:00<?, ?it/s]/tmp/ipykernel_1359663/2147759681.py:135: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():  # Включение смешанной точности\n",
      "\u001b[1;38;5;214mCOMET WARNING:\u001b[0m To get all data logged automatically, import comet_ml before the following modules: sklearn, torch.\n",
      "\u001b[1;38;5;214mCOMET WARNING:\u001b[0m As you are running in a Jupyter environment, you will need to call `experiment.end()` when finished to ensure all metrics and code are logged before exiting.\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m Couldn't find a Git repository in '/home/maindev/new_life/for_server_gen' nor in any parent directory. Set `COMET_GIT_DIRECTORY` if your Git Repository is elsewhere.\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m Experiment is live on comet.com https://www.comet.com/exp-4-try/60-1fr/309eee8349fb4ae6ad768b616605d0b5\n",
      "\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m ---------------------------------------------------------------------------------------\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m Comet.ml Experiment Summary\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m ---------------------------------------------------------------------------------------\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m   Data:\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     display_summary_level : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     name                  : above_seed_5702\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     url                   : https://www.comet.com/exp-4-try/60-1fr/309eee8349fb4ae6ad768b616605d0b5\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m   Metrics:\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     flops           : 2749424288.0\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     mean_mae        : 0.050151072442531586\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     mean_mape       : 0.14283154904842377\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     mean_r2         : 0.8904583962845831\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     mean_train_loss : 0.003681484028160319\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     mean_val_loss   : 0.004172444809228182\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m   Parameters:\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     batch_size      : 16\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     dropout_rate    : 0.3452245601536304\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     filters_1       : 4\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     hidden_size     : 91\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     kernel_size_1   : 5\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     kernel_size_2   : 4\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     learning_rate   : 0.0002582635538484884\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     num_layers      : 3\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     padding_1       : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     padding_2       : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     stride_1        : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     stride_2        : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     use_second_conv : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m   Uploads:\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     environment details : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     filename            : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     installed packages  : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     notebook            : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     os packages         : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     source_code         : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m \n",
      "\u001b[1;38;5;214mCOMET WARNING:\u001b[0m To get all data logged automatically, import comet_ml before the following modules: sklearn, torch.\n",
      "[I 2024-11-17 22:07:08,858] Trial 1 finished with values: [0.14283154904842377, 2749424288.0] and parameters: {'use_second_conv': False, 'batch_size': 16, 'filters_1': 4, 'hidden_size': 91, 'num_layers': 3, 'dropout_rate': 0.3452245601536304, 'kernel_size_1': 5, 'kernel_size_2': 4, 'stride_1': 1, 'stride_2': 1, 'padding_1': 1, 'padding_2': 1, 'learning_rate': 0.0002582635538484884}. \n",
      "/tmp/ipykernel_1359663/2147759681.py:129: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler()  # Инициализация GradScaler для масштабирования градиентов\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]/tmp/ipykernel_1359663/2147759681.py:135: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():  # Включение смешанной точности\n",
      "/tmp/ipykernel_1359663/2147759681.py:129: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler()  # Инициализация GradScaler для масштабирования градиентов\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]/tmp/ipykernel_1359663/2147759681.py:135: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():  # Включение смешанной точности\n",
      "\u001b[1;38;5;214mCOMET WARNING:\u001b[0m To get all data logged automatically, import comet_ml before the following modules: sklearn, torch.\n",
      "\u001b[1;38;5;214mCOMET WARNING:\u001b[0m As you are running in a Jupyter environment, you will need to call `experiment.end()` when finished to ensure all metrics and code are logged before exiting.\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m Couldn't find a Git repository in '/home/maindev/new_life/for_server_gen' nor in any parent directory. Set `COMET_GIT_DIRECTORY` if your Git Repository is elsewhere.\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m Experiment is live on comet.com https://www.comet.com/exp-4-try/60-1fr/15e462deafed41838293c6630ae52444\n",
      "\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m ---------------------------------------------------------------------------------------\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m Comet.ml Experiment Summary\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m ---------------------------------------------------------------------------------------\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m   Data:\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     display_summary_level : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     name                  : intact_subcompact_6359\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     url                   : https://www.comet.com/exp-4-try/60-1fr/15e462deafed41838293c6630ae52444\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m   Metrics:\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     flops           : 2726484804.0\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     mean_mae        : 0.05330413952469826\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     mean_mape       : 0.16612857580184937\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     mean_r2         : 0.8769871920740797\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     mean_train_loss : 0.004005869314413896\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     mean_val_loss   : 0.0048408623394098575\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m   Parameters:\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     batch_size      : 6\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     dropout_rate    : 0.21162801898792516\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     filters_1       : 6\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     filters_2       : 18\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     hidden_size     : 153\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     kernel_size_1   : 4\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     kernel_size_2   : 4\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     learning_rate   : 2.253159383877661e-05\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     num_layers      : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     padding_1       : 2\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     padding_2       : 2\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     stride_1        : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     stride_2        : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     use_second_conv : True\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m   Uploads:\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     environment details : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     filename            : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     installed packages  : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     notebook            : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     os packages         : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     source_code         : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m \n",
      "\u001b[1;38;5;214mCOMET WARNING:\u001b[0m To get all data logged automatically, import comet_ml before the following modules: sklearn, torch.\n",
      "[I 2024-11-17 22:11:39,345] Trial 2 finished with values: [0.16612857580184937, 2726484804.0] and parameters: {'use_second_conv': True, 'batch_size': 6, 'filters_1': 6, 'filters_2': 18, 'hidden_size': 153, 'num_layers': 1, 'dropout_rate': 0.21162801898792516, 'kernel_size_1': 4, 'kernel_size_2': 4, 'stride_1': 1, 'stride_2': 1, 'padding_1': 2, 'padding_2': 2, 'learning_rate': 2.253159383877661e-05}. \n",
      "/tmp/ipykernel_1359663/2147759681.py:129: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler()  # Инициализация GradScaler для масштабирования градиентов\n",
      "  0%|                                                    | 0/19 [00:00<?, ?it/s]/tmp/ipykernel_1359663/2147759681.py:135: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():  # Включение смешанной точности\n",
      "/tmp/ipykernel_1359663/2147759681.py:129: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler()  # Инициализация GradScaler для масштабирования градиентов\n",
      "  0%|                                                    | 0/19 [00:00<?, ?it/s]/tmp/ipykernel_1359663/2147759681.py:135: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():  # Включение смешанной точности\n",
      "\u001b[1;38;5;214mCOMET WARNING:\u001b[0m To get all data logged automatically, import comet_ml before the following modules: sklearn, torch.\n",
      "\u001b[1;38;5;214mCOMET WARNING:\u001b[0m As you are running in a Jupyter environment, you will need to call `experiment.end()` when finished to ensure all metrics and code are logged before exiting.\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m Experiment is live on comet.com https://www.comet.com/exp-4-try/60-1fr/28ef809cde8e4bf0aac89038f25178b0\n",
      "\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m Couldn't find a Git repository in '/home/maindev/new_life/for_server_gen' nor in any parent directory. Set `COMET_GIT_DIRECTORY` if your Git Repository is elsewhere.\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m ---------------------------------------------------------------------------------------\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m Comet.ml Experiment Summary\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m ---------------------------------------------------------------------------------------\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m   Data:\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     display_summary_level : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     name                  : influential_dinosaur_7335\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     url                   : https://www.comet.com/exp-4-try/60-1fr/28ef809cde8e4bf0aac89038f25178b0\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m   Metrics:\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     flops           : 2154935616.0\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     mean_mae        : 0.17019009590148926\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     mean_mape       : 0.6022658348083496\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     mean_r2         : -0.006778068369502633\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     mean_train_loss : 0.043269758115885626\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     mean_val_loss   : 0.04331267077033803\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m   Parameters:\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     batch_size      : 16\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     dropout_rate    : 0.1982175514295812\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     filters_1       : 4\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     hidden_size     : 74\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     kernel_size_1   : 4\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     kernel_size_2   : 3\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     learning_rate   : 0.0024631772885496694\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     num_layers      : 3\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     padding_1       : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     padding_2       : 2\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     stride_1        : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     stride_2        : 2\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     use_second_conv : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m   Uploads:\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     environment details : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     filename            : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     installed packages  : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     notebook            : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     os packages         : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     source_code         : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m \n",
      "\u001b[1;38;5;214mCOMET WARNING:\u001b[0m To get all data logged automatically, import comet_ml before the following modules: sklearn, torch.\n",
      "[I 2024-11-17 22:13:38,688] Trial 3 finished with values: [0.6022658348083496, 2154935616.0] and parameters: {'use_second_conv': False, 'batch_size': 16, 'filters_1': 4, 'hidden_size': 74, 'num_layers': 3, 'dropout_rate': 0.1982175514295812, 'kernel_size_1': 4, 'kernel_size_2': 3, 'stride_1': 1, 'stride_2': 2, 'padding_1': 1, 'padding_2': 2, 'learning_rate': 0.0024631772885496694}. \n",
      "/tmp/ipykernel_1359663/2147759681.py:129: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler()  # Инициализация GradScaler для масштабирования градиентов\n",
      "  0%|                                                    | 0/74 [00:00<?, ?it/s]/tmp/ipykernel_1359663/2147759681.py:135: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():  # Включение смешанной точности\n",
      "/tmp/ipykernel_1359663/2147759681.py:129: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler()  # Инициализация GradScaler для масштабирования градиентов\n",
      "  0%|                                                    | 0/74 [00:00<?, ?it/s]/tmp/ipykernel_1359663/2147759681.py:135: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():  # Включение смешанной точности\n",
      "\u001b[1;38;5;214mCOMET WARNING:\u001b[0m To get all data logged automatically, import comet_ml before the following modules: sklearn, torch.\n",
      "\u001b[1;38;5;214mCOMET WARNING:\u001b[0m As you are running in a Jupyter environment, you will need to call `experiment.end()` when finished to ensure all metrics and code are logged before exiting.\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m Experiment is live on comet.com https://www.comet.com/exp-4-try/60-1fr/985748bcc04148098690cf3082eaa855\n",
      "\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m Couldn't find a Git repository in '/home/maindev/new_life/for_server_gen' nor in any parent directory. Set `COMET_GIT_DIRECTORY` if your Git Repository is elsewhere.\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m ---------------------------------------------------------------------------------------\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m Comet.ml Experiment Summary\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m ---------------------------------------------------------------------------------------\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m   Data:\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     display_summary_level : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     name                  : bored_soldier_9221\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     url                   : https://www.comet.com/exp-4-try/60-1fr/985748bcc04148098690cf3082eaa855\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m   Metrics:\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     flops           : 117246532.0\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     mean_mae        : 0.06800024956464767\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     mean_mape       : 0.19738581776618958\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     mean_r2         : 0.8022550192142174\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     mean_train_loss : 0.00669086575665969\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     mean_val_loss   : 0.0073959910886962025\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m   Parameters:\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     batch_size      : 4\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     dropout_rate    : 0.41969770105735715\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     filters_1       : 5\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     hidden_size     : 52\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     kernel_size_1   : 3\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     kernel_size_2   : 4\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     learning_rate   : 0.0009478784337627767\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     num_layers      : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     padding_1       : 2\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     padding_2       : 2\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     stride_1        : 2\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     stride_2        : 2\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     use_second_conv : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m   Uploads:\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     environment details : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     filename            : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     installed packages  : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     notebook            : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     os packages         : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     source_code         : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m \n",
      "\u001b[1;38;5;214mCOMET WARNING:\u001b[0m To get all data logged automatically, import comet_ml before the following modules: sklearn, torch.\n",
      "[I 2024-11-17 22:15:36,086] Trial 4 finished with values: [0.19738581776618958, 117246532.0] and parameters: {'use_second_conv': False, 'batch_size': 4, 'filters_1': 5, 'hidden_size': 52, 'num_layers': 1, 'dropout_rate': 0.41969770105735715, 'kernel_size_1': 3, 'kernel_size_2': 4, 'stride_1': 2, 'stride_2': 2, 'padding_1': 2, 'padding_2': 2, 'learning_rate': 0.0009478784337627767}. \n",
      "/tmp/ipykernel_1359663/2147759681.py:129: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler()  # Инициализация GradScaler для масштабирования градиентов\n",
      "  0%|                                                    | 0/74 [00:00<?, ?it/s]/tmp/ipykernel_1359663/2147759681.py:135: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():  # Включение смешанной точности\n",
      "/tmp/ipykernel_1359663/2147759681.py:129: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler()  # Инициализация GradScaler для масштабирования градиентов\n",
      "  0%|                                                    | 0/74 [00:00<?, ?it/s]/tmp/ipykernel_1359663/2147759681.py:135: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():  # Включение смешанной точности\n",
      "\u001b[1;38;5;214mCOMET WARNING:\u001b[0m To get all data logged automatically, import comet_ml before the following modules: sklearn, torch.\n",
      "\u001b[1;38;5;214mCOMET WARNING:\u001b[0m As you are running in a Jupyter environment, you will need to call `experiment.end()` when finished to ensure all metrics and code are logged before exiting.\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m Experiment is live on comet.com https://www.comet.com/exp-4-try/60-1fr/bd9f329a05ab45e4838c7c34ce4b3694\n",
      "\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m Couldn't find a Git repository in '/home/maindev/new_life/for_server_gen' nor in any parent directory. Set `COMET_GIT_DIRECTORY` if your Git Repository is elsewhere.\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m ---------------------------------------------------------------------------------------\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m Comet.ml Experiment Summary\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m ---------------------------------------------------------------------------------------\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m   Data:\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     display_summary_level : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     name                  : difficult_actuary_6051\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     url                   : https://www.comet.com/exp-4-try/60-1fr/bd9f329a05ab45e4838c7c34ce4b3694\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m   Metrics:\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     flops           : 69608064.0\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     mean_mae        : 0.009175502695143223\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     mean_mape       : 0.02545739710330963\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     mean_r2         : 0.9962091636487804\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     mean_train_loss : 8.231030811750038e-05\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     mean_val_loss   : 0.000151562416505254\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m   Parameters:\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     batch_size      : 4\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     dropout_rate    : 0.34722202792370915\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     filters_1       : 3\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     filters_2       : 2\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     hidden_size     : 232\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     kernel_size_1   : 3\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     kernel_size_2   : 5\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     learning_rate   : 0.0006301699520804021\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     num_layers      : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     padding_1       : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     padding_2       : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     stride_1        : 2\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     stride_2        : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     use_second_conv : True\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m   Uploads:\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     environment details : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     filename            : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     installed packages  : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     notebook            : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     os packages         : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     source_code         : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m \n",
      "\u001b[1;38;5;214mCOMET WARNING:\u001b[0m To get all data logged automatically, import comet_ml before the following modules: sklearn, torch.\n",
      "[I 2024-11-17 22:17:30,890] Trial 5 finished with values: [0.02545739710330963, 69608064.0] and parameters: {'use_second_conv': True, 'batch_size': 4, 'filters_1': 3, 'filters_2': 2, 'hidden_size': 232, 'num_layers': 1, 'dropout_rate': 0.34722202792370915, 'kernel_size_1': 3, 'kernel_size_2': 5, 'stride_1': 2, 'stride_2': 1, 'padding_1': 1, 'padding_2': 1, 'learning_rate': 0.0006301699520804021}. \n",
      "/tmp/ipykernel_1359663/2147759681.py:129: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler()  # Инициализация GradScaler для масштабирования градиентов\n",
      "  0%|                                                    | 0/19 [00:00<?, ?it/s]/tmp/ipykernel_1359663/2147759681.py:135: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():  # Включение смешанной точности\n",
      "/tmp/ipykernel_1359663/2147759681.py:129: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler()  # Инициализация GradScaler для масштабирования градиентов\n",
      "  0%|                                                    | 0/19 [00:00<?, ?it/s]/tmp/ipykernel_1359663/2147759681.py:135: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():  # Включение смешанной точности\n",
      "\u001b[1;38;5;214mCOMET WARNING:\u001b[0m To get all data logged automatically, import comet_ml before the following modules: sklearn, torch.\n",
      "\u001b[1;38;5;214mCOMET WARNING:\u001b[0m As you are running in a Jupyter environment, you will need to call `experiment.end()` when finished to ensure all metrics and code are logged before exiting.\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m Experiment is live on comet.com https://www.comet.com/exp-4-try/60-1fr/2bf0f8017a51443ebced88bc2ef9b5de\n",
      "\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m Couldn't find a Git repository in '/home/maindev/new_life/for_server_gen' nor in any parent directory. Set `COMET_GIT_DIRECTORY` if your Git Repository is elsewhere.\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m ---------------------------------------------------------------------------------------\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m Comet.ml Experiment Summary\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m ---------------------------------------------------------------------------------------\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m   Data:\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     display_summary_level : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     name                  : mighty_cabana_3864\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     url                   : https://www.comet.com/exp-4-try/60-1fr/2bf0f8017a51443ebced88bc2ef9b5de\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m   Metrics:\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     flops           : 3900957024.0\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     mean_mae        : 0.06785272061824799\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     mean_mape       : 0.21997568011283875\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     mean_r2         : 0.8068956633684881\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     mean_train_loss : 0.006299210825571949\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     mean_val_loss   : 0.007777987104067105\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m   Parameters:\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     batch_size      : 16\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     dropout_rate    : 0.4588959068472044\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     filters_1       : 11\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     hidden_size     : 233\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     kernel_size_1   : 3\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     kernel_size_2   : 5\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     learning_rate   : 1.5720563458084615e-05\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     num_layers      : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     padding_1       : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     padding_2       : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     stride_1        : 2\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     stride_2        : 2\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     use_second_conv : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m   Uploads:\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     environment details : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     filename            : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     installed packages  : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     notebook            : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     os packages         : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     source_code         : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m \n",
      "\u001b[1;38;5;214mCOMET WARNING:\u001b[0m To get all data logged automatically, import comet_ml before the following modules: sklearn, torch.\n",
      "[I 2024-11-17 22:19:33,506] Trial 6 finished with values: [0.21997568011283875, 3900957024.0] and parameters: {'use_second_conv': False, 'batch_size': 16, 'filters_1': 11, 'hidden_size': 233, 'num_layers': 1, 'dropout_rate': 0.4588959068472044, 'kernel_size_1': 3, 'kernel_size_2': 5, 'stride_1': 2, 'stride_2': 2, 'padding_1': 1, 'padding_2': 1, 'learning_rate': 1.5720563458084615e-05}. \n",
      "/tmp/ipykernel_1359663/2147759681.py:129: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler()  # Инициализация GradScaler для масштабирования градиентов\n",
      "  0%|                                                    | 0/37 [00:00<?, ?it/s]/tmp/ipykernel_1359663/2147759681.py:135: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():  # Включение смешанной точности\n",
      "/tmp/ipykernel_1359663/2147759681.py:129: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler()  # Инициализация GradScaler для масштабирования градиентов\n",
      "  0%|                                                    | 0/37 [00:00<?, ?it/s]/tmp/ipykernel_1359663/2147759681.py:135: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():  # Включение смешанной точности\n",
      "\u001b[1;38;5;214mCOMET WARNING:\u001b[0m To get all data logged automatically, import comet_ml before the following modules: sklearn, torch.\n",
      "\u001b[1;38;5;214mCOMET WARNING:\u001b[0m As you are running in a Jupyter environment, you will need to call `experiment.end()` when finished to ensure all metrics and code are logged before exiting.\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m Experiment is live on comet.com https://www.comet.com/exp-4-try/60-1fr/3d3f9201fead405dbb0d3ff754cf8c05\n",
      "\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m Couldn't find a Git repository in '/home/maindev/new_life/for_server_gen' nor in any parent directory. Set `COMET_GIT_DIRECTORY` if your Git Repository is elsewhere.\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m ---------------------------------------------------------------------------------------\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m Comet.ml Experiment Summary\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m ---------------------------------------------------------------------------------------\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m   Data:\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     display_summary_level : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     name                  : overseas_jig_8867\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     url                   : https://www.comet.com/exp-4-try/60-1fr/3d3f9201fead405dbb0d3ff754cf8c05\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m   Metrics:\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     flops           : 146653080.0\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     mean_mae        : 0.1618223488330841\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     mean_mape       : 0.5589831471443176\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     mean_r2         : 0.07195564458575104\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     mean_train_loss : 0.037234129390474095\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     mean_val_loss   : 0.03782722432229479\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m   Parameters:\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     batch_size      : 8\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     dropout_rate    : 0.39835322951227636\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     filters_1       : 7\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     hidden_size     : 16\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     kernel_size_1   : 3\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     kernel_size_2   : 4\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     learning_rate   : 3.588778088006099e-05\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     num_layers      : 2\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     padding_1       : 2\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     padding_2       : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     stride_1        : 2\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     stride_2        : 2\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     use_second_conv : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m   Uploads:\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     environment details : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     filename            : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     installed packages  : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     notebook            : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     os packages         : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     source_code         : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m \n",
      "\u001b[1;38;5;214mCOMET WARNING:\u001b[0m To get all data logged automatically, import comet_ml before the following modules: sklearn, torch.\n",
      "[I 2024-11-17 22:21:04,702] Trial 7 finished with values: [0.5589831471443176, 146653080.0] and parameters: {'use_second_conv': False, 'batch_size': 8, 'filters_1': 7, 'hidden_size': 16, 'num_layers': 2, 'dropout_rate': 0.39835322951227636, 'kernel_size_1': 3, 'kernel_size_2': 4, 'stride_1': 2, 'stride_2': 2, 'padding_1': 2, 'padding_2': 1, 'learning_rate': 3.588778088006099e-05}. \n",
      "/tmp/ipykernel_1359663/2147759681.py:129: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler()  # Инициализация GradScaler для масштабирования градиентов\n",
      "  0%|                                                    | 0/37 [00:00<?, ?it/s]/tmp/ipykernel_1359663/2147759681.py:135: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():  # Включение смешанной точности\n",
      "/tmp/ipykernel_1359663/2147759681.py:129: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler()  # Инициализация GradScaler для масштабирования градиентов\n",
      "  0%|                                                    | 0/37 [00:00<?, ?it/s]/tmp/ipykernel_1359663/2147759681.py:135: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():  # Включение смешанной точности\n",
      "\u001b[1;38;5;214mCOMET WARNING:\u001b[0m To get all data logged automatically, import comet_ml before the following modules: sklearn, torch.\n",
      "\u001b[1;38;5;214mCOMET WARNING:\u001b[0m As you are running in a Jupyter environment, you will need to call `experiment.end()` when finished to ensure all metrics and code are logged before exiting.\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m Experiment is live on comet.com https://www.comet.com/exp-4-try/60-1fr/c7a2e032e3f2423eae1556b335760c4c\n",
      "\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m Couldn't find a Git repository in '/home/maindev/new_life/for_server_gen' nor in any parent directory. Set `COMET_GIT_DIRECTORY` if your Git Repository is elsewhere.\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m ---------------------------------------------------------------------------------------\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m Comet.ml Experiment Summary\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m ---------------------------------------------------------------------------------------\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m   Data:\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     display_summary_level : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     name                  : collective_date_6374\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     url                   : https://www.comet.com/exp-4-try/60-1fr/c7a2e032e3f2423eae1556b335760c4c\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m   Metrics:\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     flops           : 573119504.0\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     mean_mae        : 0.13171160221099854\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     mean_mape       : 0.45815014839172363\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     mean_r2         : 0.33156100722581894\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     mean_train_loss : 0.025900524974627012\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     mean_val_loss   : 0.026990655014070414\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m   Parameters:\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     batch_size      : 8\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     dropout_rate    : 0.10145492910316341\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     filters_1       : 2\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     hidden_size     : 71\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     kernel_size_1   : 5\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     kernel_size_2   : 5\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     learning_rate   : 1.8482009031607836e-05\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     num_layers      : 3\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     padding_1       : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     padding_2       : 2\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     stride_1        : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     stride_2        : 2\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     use_second_conv : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m   Uploads:\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     environment details : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     filename            : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     installed packages  : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     notebook            : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     os packages         : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     source_code         : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m \n",
      "\u001b[1;38;5;214mCOMET WARNING:\u001b[0m To get all data logged automatically, import comet_ml before the following modules: sklearn, torch.\n",
      "[I 2024-11-17 22:23:25,942] Trial 8 finished with values: [0.45815014839172363, 573119504.0] and parameters: {'use_second_conv': False, 'batch_size': 8, 'filters_1': 2, 'hidden_size': 71, 'num_layers': 3, 'dropout_rate': 0.10145492910316341, 'kernel_size_1': 5, 'kernel_size_2': 5, 'stride_1': 1, 'stride_2': 2, 'padding_1': 1, 'padding_2': 2, 'learning_rate': 1.8482009031607836e-05}. \n",
      "/tmp/ipykernel_1359663/2147759681.py:129: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler()  # Инициализация GradScaler для масштабирования градиентов\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]/tmp/ipykernel_1359663/2147759681.py:135: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():  # Включение смешанной точности\n",
      "/tmp/ipykernel_1359663/2147759681.py:129: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler()  # Инициализация GradScaler для масштабирования градиентов\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]/tmp/ipykernel_1359663/2147759681.py:135: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():  # Включение смешанной точности\n",
      "\u001b[1;38;5;214mCOMET WARNING:\u001b[0m To get all data logged automatically, import comet_ml before the following modules: sklearn, torch.\n",
      "\u001b[1;38;5;214mCOMET WARNING:\u001b[0m As you are running in a Jupyter environment, you will need to call `experiment.end()` when finished to ensure all metrics and code are logged before exiting.\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m Experiment is live on comet.com https://www.comet.com/exp-4-try/60-1fr/f418d8b440f149a085e3e18693d3ba76\n",
      "\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m Couldn't find a Git repository in '/home/maindev/new_life/for_server_gen' nor in any parent directory. Set `COMET_GIT_DIRECTORY` if your Git Repository is elsewhere.\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m ---------------------------------------------------------------------------------------\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m Comet.ml Experiment Summary\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m ---------------------------------------------------------------------------------------\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m   Data:\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     display_summary_level : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     name                  : electronic_guava_3110\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     url                   : https://www.comet.com/exp-4-try/60-1fr/f418d8b440f149a085e3e18693d3ba76\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m   Metrics:\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     flops           : 501282624.0\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     mean_mae        : 0.02017313614487648\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     mean_mape       : 0.056749261915683746\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     mean_r2         : 0.9824108261314438\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     mean_train_loss : 0.000616567356855277\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     mean_val_loss   : 0.0006982622437042942\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m   Parameters:\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     batch_size      : 6\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     dropout_rate    : 0.12823242712200741\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     filters_1       : 6\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     filters_2       : 3\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     hidden_size     : 214\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     kernel_size_1   : 5\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     kernel_size_2   : 4\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     learning_rate   : 0.005988670369060501\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     num_layers      : 2\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     padding_1       : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     padding_2       : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     stride_1        : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     stride_2        : 2\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     use_second_conv : True\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m   Uploads:\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     environment details : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     filename            : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     installed packages  : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     notebook            : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     os packages         : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     source_code         : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m \n",
      "\u001b[1;38;5;214mCOMET WARNING:\u001b[0m To get all data logged automatically, import comet_ml before the following modules: sklearn, torch.\n",
      "[I 2024-11-17 22:25:29,831] Trial 9 finished with values: [0.056749261915683746, 501282624.0] and parameters: {'use_second_conv': True, 'batch_size': 6, 'filters_1': 6, 'filters_2': 3, 'hidden_size': 214, 'num_layers': 2, 'dropout_rate': 0.12823242712200741, 'kernel_size_1': 5, 'kernel_size_2': 4, 'stride_1': 1, 'stride_2': 2, 'padding_1': 1, 'padding_2': 1, 'learning_rate': 0.005988670369060501}. \n",
      "/tmp/ipykernel_1359663/2147759681.py:129: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler()  # Инициализация GradScaler для масштабирования градиентов\n",
      "  0%|                                                    | 0/74 [00:00<?, ?it/s]/tmp/ipykernel_1359663/2147759681.py:135: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():  # Включение смешанной точности\n",
      "/tmp/ipykernel_1359663/2147759681.py:129: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler()  # Инициализация GradScaler для масштабирования градиентов\n",
      "  0%|                                                    | 0/74 [00:00<?, ?it/s]/tmp/ipykernel_1359663/2147759681.py:135: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():  # Включение смешанной точности\n",
      "\u001b[1;38;5;214mCOMET WARNING:\u001b[0m To get all data logged automatically, import comet_ml before the following modules: sklearn, torch.\n",
      "\u001b[1;38;5;214mCOMET WARNING:\u001b[0m As you are running in a Jupyter environment, you will need to call `experiment.end()` when finished to ensure all metrics and code are logged before exiting.\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m Experiment is live on comet.com https://www.comet.com/exp-4-try/60-1fr/5f2b3ca83a404de9a7bb948596de5791\n",
      "\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m Couldn't find a Git repository in '/home/maindev/new_life/for_server_gen' nor in any parent directory. Set `COMET_GIT_DIRECTORY` if your Git Repository is elsewhere.\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m ---------------------------------------------------------------------------------------\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m Comet.ml Experiment Summary\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m ---------------------------------------------------------------------------------------\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m   Data:\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     display_summary_level : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     name                  : strategic_tower_9772\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     url                   : https://www.comet.com/exp-4-try/60-1fr/5f2b3ca83a404de9a7bb948596de5791\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m   Metrics:\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     flops           : 832690776.0\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     mean_mae        : 0.047245848923921585\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     mean_mape       : 0.14182192087173462\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     mean_r2         : 0.8904048530933962\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     mean_train_loss : 0.0037891261255905287\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     mean_val_loss   : 0.004263662495571426\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m   Parameters:\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     batch_size      : 4\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     dropout_rate    : 0.37762903605533593\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     filters_1       : 15\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     hidden_size     : 141\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     kernel_size_1   : 3\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     kernel_size_2   : 4\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     learning_rate   : 0.002315119001631014\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     num_layers      : 2\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     padding_1       : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     padding_2       : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     stride_1        : 2\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     stride_2        : 2\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     use_second_conv : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m   Uploads:\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     environment details : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     filename            : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     installed packages  : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     notebook            : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     os packages         : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     source_code         : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m \n",
      "\u001b[1;38;5;214mCOMET WARNING:\u001b[0m To get all data logged automatically, import comet_ml before the following modules: sklearn, torch.\n",
      "[I 2024-11-17 22:29:05,636] Trial 10 finished with values: [0.14182192087173462, 832690776.0] and parameters: {'use_second_conv': False, 'batch_size': 4, 'filters_1': 15, 'hidden_size': 141, 'num_layers': 2, 'dropout_rate': 0.37762903605533593, 'kernel_size_1': 3, 'kernel_size_2': 4, 'stride_1': 2, 'stride_2': 2, 'padding_1': 1, 'padding_2': 1, 'learning_rate': 0.002315119001631014}. \n",
      "/tmp/ipykernel_1359663/2147759681.py:129: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler()  # Инициализация GradScaler для масштабирования градиентов\n",
      "  0%|                                                    | 0/30 [00:00<?, ?it/s]/tmp/ipykernel_1359663/2147759681.py:135: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():  # Включение смешанной точности\n",
      "/tmp/ipykernel_1359663/2147759681.py:129: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler()  # Инициализация GradScaler для масштабирования градиентов\n",
      "  0%|                                                    | 0/30 [00:00<?, ?it/s]/tmp/ipykernel_1359663/2147759681.py:135: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():  # Включение смешанной точности\n",
      "\u001b[1;38;5;214mCOMET WARNING:\u001b[0m To get all data logged automatically, import comet_ml before the following modules: sklearn, torch.\n",
      "\u001b[1;38;5;214mCOMET WARNING:\u001b[0m As you are running in a Jupyter environment, you will need to call `experiment.end()` when finished to ensure all metrics and code are logged before exiting.\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m Experiment is live on comet.com https://www.comet.com/exp-4-try/60-1fr/00a65b13a8a549569316f878b01a25bd\n",
      "\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m Couldn't find a Git repository in '/home/maindev/new_life/for_server_gen' nor in any parent directory. Set `COMET_GIT_DIRECTORY` if your Git Repository is elsewhere.\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m ---------------------------------------------------------------------------------------\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m Comet.ml Experiment Summary\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m ---------------------------------------------------------------------------------------\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m   Data:\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     display_summary_level : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     name                  : nearby_neutron_5638\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     url                   : https://www.comet.com/exp-4-try/60-1fr/00a65b13a8a549569316f878b01a25bd\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m   Metrics:\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     flops           : 1695475500.0\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     mean_mae        : 0.036658015102148056\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     mean_mape       : 0.10614459216594696\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     mean_r2         : 0.9431478283500938\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     mean_train_loss : 0.0016879673595382374\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     mean_val_loss   : 0.002177977485590125\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m   Parameters:\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     batch_size      : 10\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     dropout_rate    : 0.2294552338275771\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     filters_1       : 11\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     hidden_size     : 151\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     kernel_size_1   : 4\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     kernel_size_2   : 3\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     learning_rate   : 0.0009682828707094957\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     num_layers      : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     padding_1       : 2\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     padding_2       : 2\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     stride_1        : 2\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     stride_2        : 2\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     use_second_conv : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m   Uploads:\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     environment details : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     filename            : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     installed packages  : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     notebook            : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     os packages         : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     source_code         : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m \n",
      "\u001b[1;38;5;214mCOMET WARNING:\u001b[0m To get all data logged automatically, import comet_ml before the following modules: sklearn, torch.\n",
      "[I 2024-11-17 22:30:57,239] Trial 11 finished with values: [0.10614459216594696, 1695475500.0] and parameters: {'use_second_conv': False, 'batch_size': 10, 'filters_1': 11, 'hidden_size': 151, 'num_layers': 1, 'dropout_rate': 0.2294552338275771, 'kernel_size_1': 4, 'kernel_size_2': 3, 'stride_1': 2, 'stride_2': 2, 'padding_1': 2, 'padding_2': 2, 'learning_rate': 0.0009682828707094957}. \n",
      "/tmp/ipykernel_1359663/2147759681.py:129: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler()  # Инициализация GradScaler для масштабирования градиентов\n",
      "  0%|                                                    | 0/19 [00:00<?, ?it/s]/tmp/ipykernel_1359663/2147759681.py:135: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():  # Включение смешанной точности\n",
      "/tmp/ipykernel_1359663/2147759681.py:129: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler()  # Инициализация GradScaler для масштабирования градиентов\n",
      "  0%|                                                    | 0/19 [00:00<?, ?it/s]/tmp/ipykernel_1359663/2147759681.py:135: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():  # Включение смешанной точности\n",
      "\u001b[1;38;5;214mCOMET WARNING:\u001b[0m To get all data logged automatically, import comet_ml before the following modules: sklearn, torch.\n",
      "\u001b[1;38;5;214mCOMET WARNING:\u001b[0m As you are running in a Jupyter environment, you will need to call `experiment.end()` when finished to ensure all metrics and code are logged before exiting.\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m Couldn't find a Git repository in '/home/maindev/new_life/for_server_gen' nor in any parent directory. Set `COMET_GIT_DIRECTORY` if your Git Repository is elsewhere.\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m Experiment is live on comet.com https://www.comet.com/exp-4-try/60-1fr/5b75d6bcc2d74c38967c6c7647e0c0ce\n",
      "\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m ---------------------------------------------------------------------------------------\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m Comet.ml Experiment Summary\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m ---------------------------------------------------------------------------------------\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m   Data:\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     display_summary_level : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     name                  : chronic_border_5282\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     url                   : https://www.comet.com/exp-4-try/60-1fr/5b75d6bcc2d74c38967c6c7647e0c0ce\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m   Metrics:\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     flops           : 1353852432.0\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     mean_mae        : 0.14991432428359985\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     mean_mape       : 0.527289628982544\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     mean_r2         : 0.17257765317745172\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     mean_train_loss : 0.032775531980698394\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     mean_val_loss   : 0.03343934987561177\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m   Parameters:\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     batch_size      : 16\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     dropout_rate    : 0.438869316706303\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     filters_1       : 9\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     filters_2       : 13\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     hidden_size     : 58\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     kernel_size_1   : 3\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     kernel_size_2   : 3\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     learning_rate   : 2.127986299332762e-05\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     num_layers      : 2\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     padding_1       : 2\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     padding_2       : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     stride_1        : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     stride_2        : 2\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     use_second_conv : True\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m   Uploads:\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     environment details : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     filename            : 1\n"
     ]
    }
   ],
   "source": [
    "import comet_ml  # Подключаем Comet.ml для отслеживания экспериментов\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.metrics import r2_score, mean_absolute_error, mean_absolute_percentage_error\n",
    "import optuna\n",
    "from optuna import Trial\n",
    "from tqdm import tqdm\n",
    "from thop import profile  # Для подсчета FLOPs\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "import comet_ml\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from sklearn.metrics import r2_score, mean_absolute_error, mean_absolute_percentage_error\n",
    "import numpy as np\n",
    "import optuna\n",
    "from sklearn.model_selection import KFold\n",
    "import gc\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "import torch\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "import torch.profiler\n",
    "import torch\n",
    "import torch.profiler\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import torch\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "import gc\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "# Функция для освобождения памяти\n",
    "def free_memory():\n",
    "    torch.cuda.empty_cache()  # Очистка неиспользуемой памяти GPU\n",
    "    torch.cuda.ipc_collect()\n",
    "\n",
    "\n",
    "# Лог-файл для записи результатов\n",
    "log_file_path = \"600_normal_4.txt\"\n",
    "\n",
    "def log_to_file(message):\n",
    "    with open(log_file_path, \"a\") as log_file:  # Перезапись файла\n",
    "        log_file.write(message + \"\\n\")\n",
    "\n",
    "class EncDec(nn.Module):\n",
    "    def __init__(self, filters_1, filters_2, hidden_size, num_layers, dropout_rate, kernel_size_1, kernel_size_2, \n",
    "                 stride_1, stride_2, padding_1, padding_2, use_second_conv):\n",
    "        super(EncDec, self).__init__()\n",
    "        \n",
    "        self.Conv2d_1 = nn.Conv2d(in_channels=1, out_channels=filters_1, kernel_size=kernel_size_1, stride=stride_1, padding=padding_1)\n",
    "        self.BatchNorm_1 = nn.BatchNorm2d(filters_1)\n",
    "        self.MaxPool_1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        self.High = (img_size - kernel_size_1 + 2 * padding_1) // stride_1 + 1\n",
    "        self.High = (self.High - 2) // 2 + 1  # После MaxPool\n",
    "\n",
    "        self.use_second_conv = use_second_conv\n",
    "        if use_second_conv:\n",
    "            self.Conv2d_2 = nn.Conv2d(in_channels=filters_1, out_channels=filters_2, kernel_size=kernel_size_2, stride=stride_2, padding=padding_2)\n",
    "            self.BatchNorm_2 = nn.BatchNorm2d(filters_2)\n",
    "            self.MaxPool_2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "            self.High = (self.High - kernel_size_2 + 2 * padding_2) // stride_2 + 1\n",
    "            self.High = (self.High - 2) // 2 + 1 \n",
    "\n",
    "\n",
    "        conv_output_size = filters_2 if use_second_conv else filters_1\n",
    "        self.LSTM = nn.LSTM(conv_output_size * self.High * self.High, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, 210)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size = x.size(0)\n",
    "        seq_len = x.size(1)\n",
    "        \n",
    "        x = x.permute(0, 2, 1, 3, 4)\n",
    "        \n",
    "        conv2d_outputs = []\n",
    "        for i in range(seq_len):\n",
    "\n",
    "            conv2d_out = torch.relu(self.BatchNorm_1(self.Conv2d_1(x[:, :, i, :, :])))\n",
    "            conv2d_out = self.MaxPool_1(conv2d_out)\n",
    "\n",
    "        \n",
    "            if self.use_second_conv:\n",
    "                conv2d_out = torch.relu(self.BatchNorm_2(self.Conv2d_2(conv2d_out)))\n",
    "                conv2d_out = self.MaxPool_2(conv2d_out)\n",
    "        \n",
    "            conv2d_outputs.append(conv2d_out.unsqueeze(1))\n",
    "        \n",
    "        conv_outputs = torch.cat(conv2d_outputs, dim=1)\n",
    "        \n",
    "        z = conv_outputs.reshape(batch_size, seq_len, -1)\n",
    "        \n",
    "        output, (hidden, cell) = self.LSTM(z)\n",
    "        \n",
    "        pred = self.fc(output[:, -1, :])\n",
    "        \n",
    "        return self.sigmoid(pred)\n",
    "\n",
    "\n",
    "import torch\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "import gc\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "import gc\n",
    "from tqdm import tqdm\n",
    "\n",
    "def train_or_evaluate(model, loader, criterion, optimizer=None):\n",
    "\n",
    "    model = model.to(device)\n",
    "    is_train = optimizer is not None\n",
    "    if is_train:\n",
    "        model.train()\n",
    "    else:\n",
    "        model.eval()\n",
    "\n",
    "    total_loss = 0.0\n",
    "    predictions, ground_truths = [], []\n",
    "\n",
    "    scaler = GradScaler()  # Инициализация GradScaler для масштабирования градиентов\n",
    "\n",
    "    for imgs_sequence, coords_tensor in tqdm(loader, leave=False):\n",
    "        imgs_sequence = imgs_sequence.to(device, non_blocking=True)\n",
    "        coords_tensor = coords_tensor.to(device, non_blocking=True)\n",
    "\n",
    "        with autocast():  # Включение смешанной точности\n",
    "            with torch.set_grad_enabled(is_train):\n",
    "                outputs = model(imgs_sequence)\n",
    "\n",
    "                # Проверка соответствия размеров outputs и coords_tensor\n",
    "                if outputs.size() != coords_tensor.size():\n",
    "                    raise ValueError(f\"Размеры outputs {outputs.size()} и coords {coords_tensor.size()} не совпадают.\")\n",
    "\n",
    "                # Вычисляем функцию потерь\n",
    "                loss = criterion(outputs, coords_tensor)\n",
    "\n",
    "        if is_train:\n",
    "            optimizer.zero_grad()\n",
    "            scaler.scale(loss).backward()  # Масштабирование градиентов\n",
    "            scaler.step(optimizer)  # Обновление параметров\n",
    "            scaler.update()  # Обновление масштабирования\n",
    "\n",
    "        # Суммируем потери для каждого батча\n",
    "        total_loss += loss.item() * imgs_sequence.size(0)\n",
    "        predictions.extend(outputs.detach().cpu().numpy())\n",
    "        ground_truths.extend(coords_tensor.detach().cpu().numpy())\n",
    "\n",
    "    # Управление памятью\n",
    "    del imgs_sequence, coords_tensor, outputs, loss\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "    return total_loss / len(loader.dataset), predictions, ground_truths\n",
    "\n",
    "\n",
    "def calculate_flops(model, input_size):\n",
    "    \n",
    "    inputs = torch.randn(input_size).to(device)\n",
    "    flops, params = profile(model, inputs=(inputs,), verbose=False)\n",
    "    return flops\n",
    "\n",
    "def objective(trial: optuna.Trial, n_splits=2, img_size=600):\n",
    "    \"\"\"K-Fold кросс-валидация с усреднением всех метрик по фолдам.\"\"\"\n",
    "    # Гиперпараметры для текущего trial\n",
    "    use_second_conv = trial.suggest_categorical('use_second_conv', [True, False])\n",
    "    batch_size = trial.suggest_categorical('batch_size', [4, 6, 8, 10, 16])\n",
    "    filters_1 = trial.suggest_int('filters_1', 2, 16)\n",
    "    filters_2 = trial.suggest_int('filters_2', 2, 32) if use_second_conv else filters_1\n",
    "    hidden_size = trial.suggest_int('hidden_size', 4, 256)\n",
    "    num_layers = trial.suggest_int('num_layers', 1, 3)\n",
    "    dropout_rate = trial.suggest_float('dropout_rate', 0.1, 0.5)\n",
    "    kernel_size_1 = trial.suggest_int('kernel_size_1', 3, 5)\n",
    "    kernel_size_2 = trial.suggest_int('kernel_size_2', 3, 5)\n",
    "    stride_1 = trial.suggest_int('stride_1', 1, 2)\n",
    "    stride_2 = trial.suggest_int('stride_2', 1, 2)\n",
    "    padding_1 = trial.suggest_int('padding_1', 1, 2)\n",
    "    padding_2 = trial.suggest_int('padding_2', 1, 2)\n",
    "    learning_rate = trial.suggest_float('learning_rate', 1e-5, 1e-2, log=True)\n",
    "\n",
    "    # Инициализация K-Fold кросс-валидации\n",
    "    kfold = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "\n",
    "    model = EncDec(\n",
    "        filters_1=filters_1, filters_2=filters_2, hidden_size=hidden_size,\n",
    "        num_layers=num_layers, dropout_rate=dropout_rate,\n",
    "        kernel_size_1=kernel_size_1, kernel_size_2=kernel_size_2,\n",
    "        stride_1=stride_1, stride_2=stride_2,\n",
    "        padding_1=padding_1, padding_2=padding_2,\n",
    "        use_second_conv=use_second_conv\n",
    "    ).to(device)\n",
    "\n",
    "    # Массивы для хранения метрик по всем фолдам\n",
    "    all_train_losses, all_val_losses = [], []\n",
    "    all_r2_scores, all_mae_scores, all_mape_scores = [], [], []\n",
    "\n",
    "    for fold, (train_idx, val_idx) in enumerate(kfold.split(dataset)):\n",
    "        # Разделяем данные на обучающую и валидационную выборки\n",
    "        train_subset = torch.utils.data.Subset(dataset, train_idx)\n",
    "        val_subset = torch.utils.data.Subset(dataset, val_idx)\n",
    "        train_loader = DataLoader(train_subset, batch_size=batch_size, shuffle=True, num_workers=20, pin_memory=True, persistent_workers=True)\n",
    "        val_loader = DataLoader(val_subset, batch_size=batch_size, num_workers=20, pin_memory=True, persistent_workers=True)\n",
    "\n",
    "        optimizer = optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "        criterion = nn.MSELoss()\n",
    "\n",
    "        for epoch in range(num_epochs):\n",
    "            try:\n",
    "                # Обучение\n",
    "                train_loss, train_preds, train_gt = train_or_evaluate(model, train_loader, criterion, optimizer)\n",
    "\n",
    "                # Валидация (на последней эпохе)\n",
    "                if epoch == num_epochs - 1:\n",
    "                    val_loss, val_preds, val_gt = train_or_evaluate(model, val_loader, criterion)\n",
    "                    mae_val = mean_absolute_error(val_gt, val_preds)\n",
    "                    r2_val = r2_score(val_gt, val_preds)\n",
    "                    mape_val = mean_absolute_percentage_error(val_gt, val_preds)\n",
    "\n",
    "                    # Сохраняем метрики для текущего фолда\n",
    "                    all_train_losses.append(train_loss)\n",
    "                    all_val_losses.append(val_loss)\n",
    "                    all_r2_scores.append(r2_val)\n",
    "                    all_mae_scores.append(mae_val)\n",
    "                    all_mape_scores.append(mape_val)\n",
    "\n",
    "                    del val_preds, val_gt\n",
    "\n",
    "                free_memory()\n",
    "                del train_preds, train_gt\n",
    "                gc.collect()\n",
    "                torch.cuda.empty_cache()\n",
    "\n",
    "            except torch.cuda.OutOfMemoryError as e:\n",
    "                print(f\"Ошибка переполнения памяти: {str(e)}\")\n",
    "                torch.cuda.empty_cache()\n",
    "                gc.collect()\n",
    "                raise optuna.exceptions.TrialPruned()\n",
    "\n",
    "    # Усреднение метрик по всем фолдам\n",
    "    mean_train_loss = np.mean(all_train_losses)\n",
    "    mean_val_loss = np.mean(all_val_losses)\n",
    "    mean_r2 = np.mean(all_r2_scores)\n",
    "    mean_mae = np.mean(all_mae_scores)\n",
    "    mean_mape = np.mean(all_mape_scores)\n",
    "\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    free_memory()\n",
    "\n",
    "    # Логируем метрики (например, в Comet.ml)\n",
    "    experiment = comet_ml.Experiment(api_key=\"VHqjhRzLpPJbb986xCh3V3ei2\", project_name=project_name, workspace=workspace)\n",
    "    experiment.log_parameters(trial.params)\n",
    "    experiment.log_metric(\"mean_train_loss\", mean_train_loss)\n",
    "    experiment.log_metric(\"mean_val_loss\", mean_val_loss)\n",
    "    experiment.log_metric(\"mean_r2\", mean_r2)\n",
    "    experiment.log_metric(\"mean_mae\", mean_mae)\n",
    "    experiment.log_metric(\"mean_mape\", mean_mape)\n",
    "    flops = calculate_flops(model, (batch_size, 1, 1, 600, 600))\n",
    "    experiment.log_metric(\"flops\", flops, epoch=fold)\n",
    "    experiment.end()\n",
    "    experiment.end()\n",
    "\n",
    "    return mean_mape, flops\n",
    "\n",
    "\n",
    "study = optuna.create_study(directions=['minimize', 'minimize'], pruner=optuna.pruners.NopPruner())\n",
    "study.optimize(objective, n_trials=n_trials)\n",
    "torch.cuda.set_per_process_memory_fraction(0.98) \n",
    "\n",
    "best_trials = study.best_trials\n",
    "\n",
    "with open(f'{filename}', 'a') as f:\n",
    "    for trial in best_trials:\n",
    "        f.write(f\"Лучшие параметры для trial {trial.number}:\\n\")\n",
    "        for key, value in trial.params.items():\n",
    "            f.write(f\"{key}: {value}\\n\")\n",
    "    \n",
    "        f.write(f\"\\nЛучшие метрики для trial {trial.number}:\\n\")\n",
    "        f.write(f\"R² на валидации: {trial.values[0]}:\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3034f7a2-01f5-49d5-b271-3c9b556fe3b8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7893542b-f8f4-4a02-a320-7faad6a01315",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp3",
   "language": "python",
   "name": "nlp3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
